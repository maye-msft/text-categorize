{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_key = 'Azure Cognitive Service Text Analysis Subscription Key'\n",
    "text_analytics_base_url = \"https://eastasia.api.cognitive.microsoft.com/text/analytics/v2.1/\"\n",
    "csv_file_path = 'data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "def strip_html(src):\n",
    "    p = BeautifulSoup(src)\n",
    "    text = p.findAll(text=lambda text:isinstance(text, NavigableString))\n",
    "    return u\" \".join(text)\n",
    "\n",
    "key_phrase_api_url = text_analytics_base_url + \"keyPhrases\"\n",
    "print(key_phrase_api_url)\n",
    "\n",
    "entity_linking_api_url = text_analytics_base_url + \"entities\"\n",
    "print(entity_linking_api_url)\n",
    "\n",
    "headers   = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "\n",
    "def load_entities(texts):\n",
    "    res_entities=[]\n",
    "    text_list=[]\n",
    "    count=1\n",
    "    for text in texts:\n",
    "        count+=1\n",
    "        text_list.append({\"id\":count,\"text\":text})\n",
    "    documents = {'documents':text_list}\n",
    "    response  = requests.post(entity_linking_api_url, headers=headers, json=documents)\n",
    "    entities = response.json()\n",
    "#     print(json.dumps(entities, indent=4, sort_keys=True))\n",
    "    if('documents' in entities):\n",
    "        for doc in entities['documents']:\n",
    "            for ent in doc['entities']:\n",
    "                if(ent['name'] not in entities):\n",
    "                    res_entities.append(ent['name'])\n",
    "    return res_entities\n",
    "\n",
    "def load_keyphrases(texts):\n",
    "    phrases=[]\n",
    "    text_list=[]\n",
    "    count=1\n",
    "    for text in texts:\n",
    "        count+=1\n",
    "        text_list.append({\"id\":count,\"text\":text})\n",
    "    documents = {'documents':text_list}\n",
    "    response  = requests.post(key_phrase_api_url, headers=headers, json=documents)\n",
    "    keyphrases = response.json()\n",
    "    if('documents' in keyphrases):\n",
    "        for doc in keyphrases['documents']:\n",
    "            for keyphrase in doc['keyPhrases']:\n",
    "                phrases.append(keyphrase)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result={}\n",
    "#csv has three columns: id, title, description\n",
    "with open(csv_file_path, mode='r', encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        if row[0] not in result:\n",
    "            texts = strip_html(row[2]).split('\\n')\n",
    "            text_list = []\n",
    "            text_line_count = 0\n",
    "            text_line_count+=1\n",
    "            text_list.append(row[1])\n",
    "            for text in texts:\n",
    "                text_line_count+=1\n",
    "                text_list.append(text)\n",
    "            try :\n",
    "                entities = load_entities(text_list)\n",
    "                keyphrases = load_keyphrases(text_list)\n",
    "                result[row[0]] = {\"title\":row[1], \"description\":strip_html(row[2]), \"entities\":entities, \"keyphrases\":keyphrases}\n",
    "                print(row[0]+\" loaded.\")\n",
    "            except :\n",
    "                print(row[0]+\" not loaded!\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(result, open('./data_entities_keyphrases', 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional extend entities by wiki2vec\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "from wikipedia2vec.dictionary import Dictionary, Item, Word, Entity\n",
    "wiki2vec = Wikipedia2Vec.load('enwiki_20180420_100d.pkl')\n",
    "import threading\n",
    "mylock = threading.Lock() \n",
    "\n",
    "def load_entity(id, entities):\n",
    "    sub_entities = []\n",
    "    for ent in entities:\n",
    "        similar_entities = wiki2vec.most_similar(wiki2vec.get_entity(ent), 10)\n",
    "        for similar_entity in similar_entities:\n",
    "            if isinstance(similar_entity[0], Entity):\n",
    "                sub_entities.append(similar_entity[0].title)\n",
    "\n",
    "    for ent in sub_entities:\n",
    "        if(ent not in entities):\n",
    "            entities.append(ent)\n",
    "    mylock.acquire() #Get the lock    \n",
    "    try:\n",
    "        result[id]['extended_entities'] = entities\n",
    "    finally:\n",
    "        mylock.release()\n",
    "        \n",
    "import concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as e:\n",
    "    for id in result:\n",
    "        count+=1\n",
    "        entities = result[id]['entities']\n",
    "        e.submit(load_entity, id, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(result, open('./data_entities_keyphrases_allentities', 'wb'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "    'Predictive Maintenance':{\n",
    "        'entities':['Predictive maintenance', 'Anomaly detection'],\n",
    "        'keyphrases':['Anomaly', 'abnormal']\n",
    "    },\n",
    "    'Connected Factory':{\n",
    "        'entities':['SCADA', \n",
    "                    'OPC Unified Architecture', \n",
    "                    'Overall equipment effectiveness', \n",
    "                    'Remote terminal unit', \n",
    "                    'Programmable logic controller',\n",
    "                   'Manufacturing execution system'],\n",
    "        'keyphrases':['Connected Robotics']\n",
    "    },\n",
    "    'Cognitive Quality':{\n",
    "        'conditions':[\n",
    "               {\n",
    "                   'keyphrases_a':['quality'],\n",
    "                   'keyphrases_b':['computer vision', 'cognitive', 'cv', 'deep learning']\n",
    "               }\n",
    "        ]\n",
    "    },\n",
    "    'Ambient Intelligence':{\n",
    "        'conditions':[\n",
    "               {\n",
    "                   'keyphrases_a':['safety'],\n",
    "                   'keyphrases_b':['factory', 'plant', 'logistics']\n",
    "               }\n",
    "        ],\n",
    "        'entities':['Safety'],\n",
    "\n",
    "    },\n",
    "    'Service Bot':{\n",
    "        'entities':['Speech recognition', 'Chatbot'],\n",
    "        'keyphrases':['LUIS']\n",
    "    },\n",
    "    'Asset Management':{\n",
    "        'entities':['Asset Management', 'Traceability', 'Radio-frequency identification'],\n",
    "        'keyphrases':['Traceability']\n",
    "    },\n",
    "    'Mixed Reality':{\n",
    "        'entities':['Mixed reality']\n",
    "    },\n",
    "    'Process Optimization':{\n",
    "        'keyphrases':['optimization','Optimizer']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_result = {}\n",
    "for rule in rules:\n",
    "    cate_result[rule] = []\n",
    "    for id in result:\n",
    "        if 'entities' in rules[rule]:\n",
    "            for entity in rules[rule]['entities']:\n",
    "                if entity in result[id]['entities'] and eng not in cate_result[rule]:\n",
    "                    cate_result[rule].append(id)\n",
    "        if 'keyphrases' in rules[rule]:\n",
    "            wholekeyphrases = (' '.join(result[id]['keyphrases'])).lower()\n",
    "            for keyphrase in rules[rule]['keyphrases']:\n",
    "                if keyphrase.lower() in wholekeyphrases and id not in cate_result[rule]:\n",
    "                    cate_result[rule].append(id)\n",
    "        if 'conditions' in rules[rule]:\n",
    "            \n",
    "            for condition in rules[rule]['conditions']:\n",
    "                \n",
    "                if('keyphrases_a' in condition and 'keyphrases_b' in condition):\n",
    "                    wholekeyphrases = (' '.join(result[id]['keyphrases'])).lower()\n",
    "                    condition_a = False\n",
    "                    condition_b = False\n",
    "                    \n",
    "                    for keypharse_a in condition['keyphrases_a']:\n",
    "                        if keypharse_a.lower() in wholekeyphrases:\n",
    "                            condition_a = True\n",
    "                    for keypharse_b in condition['keyphrases_b']:\n",
    "                        if keypharse_b.lower() in wholekeyphrases:\n",
    "                            condition_b = True\n",
    "                    if condition_a and condition_b and id not in cate_result[rule]:\n",
    "                        cate_result[rule].append(id)\n",
    "                elif('entities_a' in condition and 'keyphrases_b' in condition):\n",
    "                    wholekeyphrases = (' '.join(result[id]['keyphrases'])).lower()\n",
    "                    condition_a = False\n",
    "                    condition_b = False\n",
    "                    \n",
    "                    for entity_a in condition['entities_a']:\n",
    "                        if entity_a in result[id]['entities']:\n",
    "                            condition_a = True\n",
    "                    for keypharse_b in condition['keyphrases_b']:\n",
    "                        if keypharse_b.lower() in wholekeyphrases:\n",
    "                            condition_b = True\n",
    "                    if condition_a and condition_b and id not in cate_result[rule]:\n",
    "                        cate_result[rule].append(id)\n",
    "                    \n",
    "\n",
    "for result in cate_result:\n",
    "    print(result +\" \"+str(len(cate_result[result])))\n",
    "    for eng in cate_result[result]:\n",
    "        print(result[id]['title'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./result_entities.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow(['id', 'entity'])\n",
    "    for id in result:\n",
    "        for entity in result[id]['entities']:\n",
    "            row=[id, entity]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./result_categories.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow(['id', 'category'])\n",
    "    for result in cate_result:\n",
    "        for id in cate_result[result]:\n",
    "            row=[id, result]\n",
    "            writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
